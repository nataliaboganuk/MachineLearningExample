# %%
%pip install pandas
%pip install scikit-learn
%pip install seaborn
%pip install matplotlib

# %%
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression

# %%
# Load the Dataset
df = pd.read_csv("Data/sample_results_dataset1.csv")
df.head()

# %%
# Mapping for A-level grades
a_level_points = {
    "A*": 60,
    "A": 50,
    "B": 40,
    "C": 30,
    "D": 20,
    "E": 10,
    "U": 0
}

df["GCE_A_Value"] = df["GCE A"].map(a_level_points)

# %%
# Mapping for AS-level grades
as_level_points = {
    "A": 25,
    "B": 20,
    "C": 15,
    "D": 10,
    "E": 5,
    "U": 0
}

df["GCE_AS_Value"] = df["GCE AS"].map(as_level_points)

# %%
# Standardize the Data (for clustering)
df[["GCE_A_ValueStan", "GCSEStan"]] = StandardScaler().fit_transform(
    df[["GCE_A_Value", "GCSE"]]
)
df.head()

# %%
# Visualise A levels and GCSE
plt.figure(figsize=(8,4))
plt.scatter(df["GCE_A_ValueStan"], df["GCSEStan"])
plt.title("GCE A and GCSE")
plt.xlabel("GCE A")
plt.ylabel("GCSE")
plt.show()

# %%
# Fit final KMeans model using chosen K
optimal_k = 3
kmeans = KMeans(
    n_clusters=optimal_k,
    init="k-means++",
    n_init="auto",
    random_state=0
)

kmeans.fit(df[["GCE_A_ValueStan", "GCSEStan"]])
df["Cluster"] = kmeans.labels_

# Plot clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(
    data=df,
    x="GCE_A_ValueStan",
    y="GCSEStan",
    hue="Cluster",
    palette="tab10",
    s=60
)

centroids = kmeans.cluster_centers_
plt.scatter(
    centroids[:, 0],
    centroids[:, 1],
    s=200,
    c="black",
    marker="X",
    label="Centroids"
)

plt.title("K-Means Clustering of Students\n(GCE A-Level vs GCSE)")
plt.xlabel("GCE A-Level (Standardized)")
plt.ylabel("GCSE (Standardized)")
plt.legend()
plt.show()

# %%
# Find the optimal K Value
import warnings
warnings.filterwarnings('ignore')

TotVar = []
Silhouette = []

StartK = 2
EndK = 15

for K in range(StartK, EndK):
    kmeans = KMeans(
        n_clusters=K,
        init="k-means++",
        n_init="auto",
        random_state=0
    )
    
    kmeans.fit(df[["GCE_A_ValueStan", "GCSEStan"]])
    labels = kmeans.labels_
    
    Silhouette.append(silhouette_score(df[["GCE_A_ValueStan", "GCSEStan"]], labels))
    TotVar.append(kmeans.inertia_)

# %%
# Elbow plot
plt.figure(figsize=(8,4))
plt.plot(range(StartK, EndK), TotVar, color="red", marker="8")
plt.xlabel("K Value")
plt.xticks(np.arange(StartK, EndK,1))
plt.ylabel("Total variation")
plt.title("K Means Elbow plot\nClusters by total variation")
plt.show()

# %%
# Silhouette plot
plt.figure(figsize=(8,4))
plt.plot(range(StartK, EndK), Silhouette, color="blue", marker="8")
plt.xlabel("K Value")
plt.xticks(np.arange(StartK, EndK,1))
plt.ylabel("Silhouette Score")
plt.title("K Means Silhouette Score\nClusters by Silhouette Score")
plt.show()

# %%
# Table of results
LoopResults = pd.DataFrame(np.arange(StartK, EndK,1), columns=["K Value"])
LoopResults["Total Variation"] = TotVar
LoopResults["Silhouette Score"] = Silhouette
print(LoopResults)

# %%
# Final K-Means using K=8
kmeans = KMeans(n_clusters=8, init="k-means++", n_init="auto", random_state=0)
df["Clusters"] = kmeans.fit_predict(df[["GCE_A_ValueStan", "GCSEStan"]])
df.head()

# %%
# Visualise the 8 Clusters
plt.figure(figsize=(8,4))
plt.scatter(df["GCE_A_ValueStan"], df["GCSEStan"], c=df["Clusters"])
plt.title("GCE A and GCSE by Cluster")
plt.xlabel("GCE_A_ValueStan")
plt.ylabel("GCSEStan")
plt.show()

# -------------------------------------------------------------------
# ðŸš€ NEW SECTION: LOGISTIC REGRESSION TO PREDICT A GRADES
# -------------------------------------------------------------------

# %%
# Create binary target: 1 = A or A*, 0 = everything else
df["A_Grade"] = df["GCE A"].isin(["A", "A*"]).astype(int)

# %%
# Select predictors for logistic regression
features = ["GCSE", "GCE_AS_Value", "Clusters"]
X = df[features]
y = df["A_Grade"]

# %%
# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# %%
# Scale ALL predictors
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# %%
# Fit Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# %%
# Evaluate Model
y_pred = log_reg.predict(X_test_scaled)

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# %%
# Add predicted probability of achieving an A grade
df["Prob_A"] = log_reg.predict_proba(scaler.transform(df[features]))[:, 1]
df[["GCE A", "Prob_A"]].head()
